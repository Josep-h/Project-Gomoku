Alpha Zero可以调整的点：

[1] **输入特征** AlphaGo/AlphaGo Zero把过往的棋局和当前棋局叠在一起作为network的输入，一方面可能是因为围棋的落子规则决定了当前落子不仅仅依赖于当前局面，也依赖于过往局面；另一方面叠加更多的过往棋局也可能利于训练。 此外，围棋的先手和后手的最终判输赢的规则也不一样，故也需要告诉network当前玩家是先手还是后手。对于五子棋而言，落子仅仅取决于当前棋局，与过往棋局完全无关，此外先后手最终的判输赢规则是一样的，所以仅仅需要输入当前局面即可。 对于11*11的棋盘，输入shape可以是[B, 2, 11, 11]，其中B是bachsize，2个channel其中一个是当前玩家的特征，另一个是对方玩家的特征，有棋子的地方为1，没有棋子的地方为0。 理论上只需要一个channel就可以的，当前玩家的棋子为1，对方玩家的棋子为-1，没有棋子的地方是0。搞两个channel可以加速收敛。此外，该project加了第三个channel，其在对方玩家最后一次落子的地方为1，其余地方为0，即表征last action。这个channel可以起到一个attention的作用，告诉当前玩家可能需要聚焦于对方玩家的落子点的附近进行落子。 这个channel可能没啥太大卵用，后续可以做对比实验试一试。

[2] **MCST树设计** 一般来说蒙特卡洛搜索树是一个树状结构，但由于五子棋的落子决策完全仅依赖于当前状态（有last action的情形除外），而不同落子顺序可能到达相同的状态，这个相同的状态的状态信息就可以复用了。 故本project并没有设计成树状结构，而是以dict的形式存储，其中key为一个字符串表示的某种状态，value是该状态的状态信息。从不同路径抵达该状态时可以共享该信息，并共同更新该信息。但是在有last action的时候，情况有些微妙的变化。 last action仅仅在需要作为network的输入的时候起作用。在模拟对弈的时候，到达某一个局面以后，假设需要以当前局面为根节点出发进行500次搜索，这500次的last action是一样的，搜索完毕以后在该节点形成的概率分布将作为policy的监督信号，该监督信号都对应于同一个last action，这一点是没有问题的。但是当该局面节点曾经作为叶子节点的时候，对叶子节点的评估所使用的last action就未必是现在的last action了。 只有当对弈局数足够多以后，这个影响才可以逐渐减弱。后续可以去掉last action这个channel一试。

[3] **数据处理** `五子棋有一个很大的bug，就是（貌似35步以内）先手必赢`。这样产生的后果是，模拟对弈的数据里面，先手赢的数据量会多于后手赢的数据量，这样失衡的数据直接拿去训练，会导致网络进一步偏好先手赢（如果当前玩家落子前，棋局里面当前玩家的棋子数量等于对手玩家的棋子数量，则当前玩家就是先手；若当前玩家的棋子数量比对手棋子数量少一个，则是后手。故网络完全可以通过棋子数量学到当前玩家是先手还是后手。），这种偏好进一步让模拟对弈产生更多的先手赢的棋局。 最终模型可能会坍塌，即先手预测的value接近于1，policy是比较准确；后手预测的value接近于-1。坍塌以后，会产生大量长度只有9或者11的棋局，这些棋局是先手很快就连成了五颗棋子，后手连成了四颗或者比较乱的摆放。当然，如果噪声足够大，训练时间足够长，这种现象可以缓解。本project采取一个的缓解方案是，记录replay buffer里面先手和后手赢棋的棋局数，当某一方赢棋数量太少的时候，若搜集到该方的一条赢棋，则重复加入buffer。此外，对于步数太短的棋局，以一定概率舍弃。这一部分在`utils.py RandomStack`里面。

[4] **训练权重** 在不断模拟对弈过程中，越是往后的的局面出现的频次就会越小，越是靠前的局面出现的频次肯定越大。故在本project设计了一个权重增长因子，使得靠后的局面获得的训练权重大于靠前的局面的训练权重。这样做的另一个原因是，靠后的落子与输赢的关联性可能更大，所以获得一个较大的训练权重。

[5] **探索方案** 在训练过程中，很容易使得网络输出的policy的熵太小，不利于充分探索。缓解方案有很多，例如把熵加入loss里面，加大噪声等等。本project比较暴力，在根节点处强制要求每个子节点至少被访问两次。这样一方面可以加大探索力度，另一方面让监督信号的概率分布smooth一些，有点类似于监督学习里面的label smooth。 此外，对于根节点加入了0.25/0.75的狄利克雷噪声，非根节点加入0.1/0.9的狄利克雷噪声。在主游戏最终选择action的时候，只根据结点访问次数的概率分布选择action，不再加入噪声。